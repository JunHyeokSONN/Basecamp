{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunHyeokSONN/Naver_Cafe/blob/main/TF2_%EC%9D%B4%EA%B1%B0%EB%8B%A4!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37ktUXcAZw5D",
        "outputId": "290f5568-d056-4662-ca1d-512f4eb69b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl (668.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 668.3 MB 18 kB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 34.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.5.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.47.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 18 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 7s (65.0 MB/s)\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155654 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "# 아래에서 UNIMPLEMENTED: DNN library is not found 오류 발생해서 tensorflow 버전을 수정함.\n",
        "!pip install tensorflow==2.8\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF8ysCfYKgTP"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "In this notebook, we implement [The TensorFlow 2 Object Detection Library](https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html) for training on your own dataset.\n",
        "\n",
        "We also recommend reading our blog post on [Train TensorFlow 2 Object Detection on custom data](https://blog.roboflow.com/train-a-tensorflow2-object-detection-model/) side by side.\n",
        "\n",
        "We will take the following steps to implement YOLOv4 on our custom data:\n",
        "* Install TensorFlow2 Object Detection Dependencies\n",
        "* Download Custom TensorFlow2 Object Detection Dataset\n",
        "* Write Custom TensorFlow2 Object Detection Training Configuation\n",
        "* Train Custom TensorFlow2 Object Detection Model\n",
        "* Export Custom TensorFlow2 Object Detection Weights\n",
        "* Use Trained TensorFlow2 Object Detection For Inference on Test Images\n",
        "\n",
        "When you are done you will have a custom detector that you can use. It will make inference like this:\n",
        "\n",
        "#### ![Roboflow Workmark](https://i.imgur.com/L0n564N.png)\n",
        "\n",
        "### **Reach out for support**\n",
        "\n",
        "If you run into any hurdles on your own data set or just want to share some cool results in your own domain, [reach out!](https://roboflow.ai) \n",
        "\n",
        "\n",
        "\n",
        "#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7EOtpvlLeS0"
      },
      "source": [
        "# Install TensorFlow2 Object Detection Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypWGYdPlLRUN",
        "outputId": "042f7c3b-020a-4f9d-e1cd-bc0f3eddf505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 3469, done.\u001b[K\n",
            "remote: Counting objects: 100% (3469/3469), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2905/2905), done.\u001b[K\n",
            "remote: Total 3469 (delta 900), reused 1490 (delta 507), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3469/3469), 46.88 MiB | 15.21 MiB/s, done.\n",
            "Resolving deltas: 100% (900/900), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QPmVBSlLTzM",
        "outputId": "1f2434d6-2bcc-45e2-bff2-a937e22fe401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/models/research\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.41.0-cp37-cp37m-manylinux2010_x86_64.whl (10.9 MB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.32)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.7.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.9.2-py2.py3-none-any.whl (2.1 MB)\n",
            "Collecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.9 MB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.8.0)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.21.6)\n",
            "Collecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "Collecting tensorflow-text~=2.9.0\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Collecting tensorflow~=2.9.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.31.6)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2022.2.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.56.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.47.0)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: pyarrow<8.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (6.0.1)\n",
            "Collecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "Collecting cloudpickle<3,>=2.1.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting protobuf<4.0.0dev,>=3.12.0\n",
            "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2022.6.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.9.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, docopt, seqeval\n",
            "  Building wheel for object-detection (setup.py): started\n",
            "  Building wheel for object-detection (setup.py): finished with status 'done'\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1694955 sha256=7939cafd54005b57cd00698a4394c0b234a24449d6dd20ffe1a47f3158d58609\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6q26u79f/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for py-cpuinfo (setup.py): started\n",
            "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=646d635028f20bac2087576d9883ffef6c4b7496ad3364665bfcf6171f050cd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for dill (setup.py): started\n",
            "  Building wheel for dill (setup.py): finished with status 'done'\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=849c03cd876c482db760630c3e541d40ac1d71e53391bafad9631616b6a0739a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py): started\n",
            "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=4cca78ec72a350c3e3a0c96ca86fbf6442824c83b927942f6efc3b946a94dcb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=896c80658a86c75bf581b48952180a9db47152dc682da4b335311251dc1b6c5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=bb13eb5d220de2c03be91648c89deda584b30f8e722be70d3b329a3fbb5e33a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo dill avro-python3 docopt seqeval\n",
            "Installing collected packages: requests, pyparsing, protobuf, tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, portalocker, docopt, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, hdfs, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pymongo\n",
            "    Found existing installation: pymongo 4.2.0\n",
            "    Uninstalling pymongo-4.2.0:\n",
            "      Successfully uninstalled pymongo-4.2.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.5.0\n",
            "    Uninstalling cloudpickle-1.5.0:\n",
            "      Successfully uninstalled cloudpickle-1.5.0\n",
            "Successfully installed apache-beam-2.41.0 avro-python3-1.10.2 cloudpickle-2.1.0 colorama-0.4.5 dill-0.3.1.1 docopt-0.6.2 fastavro-1.6.0 flatbuffers-1.12 gast-0.4.0 hdfs-2.7.0 keras-2.9.0 lvis-0.5.3 object-detection-0.1 orjson-3.8.0 portalocker-2.5.1 proto-plus-1.22.0 protobuf-3.19.4 py-cpuinfo-8.0.0 pymongo-3.12.3 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-addons-0.17.1 tensorflow-estimator-2.9.0 tensorflow-io-0.26.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.9.0 tf-models-official-2.9.2 tf-slim-1.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"
          ]
        }
      ],
      "source": [
        "# Install the Object Detection API\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wHfsJ5nWLWh9"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import imageio\n",
        "import glob\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display, Javascript\n",
        "from IPython.display import Image as IPyImage\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh_HPMOqWH9z",
        "outputId": "78d5e899-ea9e-4784-e55b-6fe8dc5d20f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running tests under Python 3.7.13: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2022-08-28 06:23:44.185217: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "W0828 06:23:44.525563 140257936017280 model_builder.py:1102] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.58s\n",
            "I0828 06:23:44.777905 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.58s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.52s\n",
            "I0828 06:23:45.295976 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.52s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.26s\n",
            "I0828 06:23:45.557315 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.26s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.35s\n",
            "I0828 06:23:45.910801 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.35s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.72s\n",
            "I0828 06:23:47.626515 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.72s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0828 06:23:47.627408 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "I0828 06:23:47.649250 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "I0828 06:23:47.663078 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "I0828 06:23:47.677593 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "I0828 06:23:47.771813 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.09s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "I0828 06:23:47.936761 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.19s\n",
            "I0828 06:23:48.129669 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.19s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "I0828 06:23:48.290795 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.14s\n",
            "I0828 06:23:48.436306 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.04s\n",
            "I0828 06:23:48.480086 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.04s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0828 06:23:48.765132 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0828 06:23:48.765385 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 64\n",
            "I0828 06:23:48.765504 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 3\n",
            "I0828 06:23:48.768961 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:48.793683 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:48.793842 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:48.885025 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:48.885262 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:49.123877 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:49.124100 140257936017280 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:23:49.358034 140257936017280 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:23:49.358261 140257936017280 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:23:50.053218 140257936017280 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:23:50.053431 140257936017280 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:23:50.482452 140257936017280 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:23:50.482663 140257936017280 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:23:51.129910 140257936017280 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:23:51.130148 140257936017280 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0828 06:23:51.247973 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0828 06:23:51.311720 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:23:51.477118 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0828 06:23:51.477346 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 88\n",
            "I0828 06:23:51.477441 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 4\n",
            "I0828 06:23:51.479801 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:51.522337 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:51.526719 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:51.751466 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:51.751664 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:52.121605 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:52.121828 140257936017280 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:23:52.566776 140257936017280 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:23:52.566989 140257936017280 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:23:53.198964 140257936017280 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:23:53.199205 140257936017280 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:23:53.849069 140257936017280 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:23:53.849291 140257936017280 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:23:54.607522 140257936017280 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:23:54.607740 140257936017280 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0828 06:23:54.842043 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0828 06:23:54.906447 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:23:55.009423 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0828 06:23:55.009630 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 112\n",
            "I0828 06:23:55.009721 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 5\n",
            "I0828 06:23:55.014170 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:55.035087 140257936017280 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:23:55.035206 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:55.285145 140257936017280 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:23:55.285355 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:55.704747 140257936017280 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:23:55.704969 140257936017280 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0828 06:23:56.200515 140257936017280 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0828 06:23:56.200714 140257936017280 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0828 06:23:56.806522 140257936017280 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0828 06:23:56.806742 140257936017280 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0828 06:23:57.466584 140257936017280 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0828 06:23:57.466796 140257936017280 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0828 06:23:58.216416 140257936017280 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0828 06:23:58.218188 140257936017280 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I0828 06:23:58.588117 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I0828 06:23:58.650205 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:23:58.767092 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0828 06:23:58.767293 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 160\n",
            "I0828 06:23:58.767367 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 6\n",
            "I0828 06:23:58.769839 140257936017280 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0828 06:23:58.808774 140257936017280 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0828 06:23:58.808949 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:23:59.401368 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:23:59.401578 140257936017280 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0828 06:23:59.880108 140257936017280 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0828 06:23:59.880289 140257936017280 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0828 06:24:00.246069 140257936017280 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0828 06:24:00.246297 140257936017280 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0828 06:24:00.863285 140257936017280 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0828 06:24:00.863501 140257936017280 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0828 06:24:01.520635 140257936017280 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0828 06:24:01.520859 140257936017280 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0828 06:24:02.561033 140257936017280 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0828 06:24:02.561261 140257936017280 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I0828 06:24:02.910765 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I0828 06:24:02.960687 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:24:03.101132 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0828 06:24:03.101351 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 224\n",
            "I0828 06:24:03.101437 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 7\n",
            "I0828 06:24:03.103916 140257936017280 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0828 06:24:03.128792 140257936017280 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0828 06:24:03.128949 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:24:03.349672 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:24:03.354169 140257936017280 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0828 06:24:03.666565 140257936017280 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0828 06:24:03.666731 140257936017280 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0828 06:24:03.970523 140257936017280 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0828 06:24:03.970696 140257936017280 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0828 06:24:04.431801 140257936017280 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0828 06:24:04.431973 140257936017280 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0828 06:24:04.886396 140257936017280 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0828 06:24:04.886564 140257936017280 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0828 06:24:05.506548 140257936017280 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0828 06:24:05.506717 140257936017280 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I0828 06:24:05.657597 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I0828 06:24:05.685254 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:24:05.759896 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0828 06:24:05.760041 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 288\n",
            "I0828 06:24:05.760122 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 7\n",
            "I0828 06:24:05.761607 140257936017280 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0828 06:24:05.776416 140257936017280 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0828 06:24:05.776521 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:24:05.958678 140257936017280 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0828 06:24:05.958855 140257936017280 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0828 06:24:06.340870 140257936017280 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0828 06:24:06.341041 140257936017280 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0828 06:24:06.924537 140257936017280 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0828 06:24:06.924710 140257936017280 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0828 06:24:07.466470 140257936017280 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0828 06:24:07.466637 140257936017280 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0828 06:24:07.996615 140257936017280 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0828 06:24:07.996787 140257936017280 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0828 06:24:08.714508 140257936017280 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0828 06:24:08.714674 140257936017280 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I0828 06:24:08.940823 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I0828 06:24:08.968598 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:24:09.054001 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0828 06:24:09.054183 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 384\n",
            "I0828 06:24:09.054264 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 8\n",
            "I0828 06:24:09.056238 140257936017280 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0828 06:24:09.072423 140257936017280 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0828 06:24:09.072550 140257936017280 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0828 06:24:09.261233 140257936017280 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0828 06:24:09.261413 140257936017280 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0828 06:24:09.719118 140257936017280 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0828 06:24:09.719288 140257936017280 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0828 06:24:10.181755 140257936017280 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0828 06:24:10.181925 140257936017280 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0828 06:24:10.796031 140257936017280 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0828 06:24:10.796211 140257936017280 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0828 06:24:11.411704 140257936017280 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0828 06:24:11.411880 140257936017280 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0828 06:24:12.265121 140257936017280 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0828 06:24:12.265283 140257936017280 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I0828 06:24:12.711045 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I0828 06:24:12.738755 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0828 06:24:12.833724 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0828 06:24:12.833871 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 384\n",
            "I0828 06:24:12.833943 140257936017280 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 8\n",
            "I0828 06:24:12.835442 140257936017280 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0828 06:24:12.851155 140257936017280 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0828 06:24:12.851268 140257936017280 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0828 06:24:13.102646 140257936017280 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0828 06:24:13.102827 140257936017280 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0828 06:24:13.637451 140257936017280 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0828 06:24:13.637619 140257936017280 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0828 06:24:14.168423 140257936017280 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0828 06:24:14.168590 140257936017280 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0828 06:24:14.917614 140257936017280 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0828 06:24:14.917783 140257936017280 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0828 06:24:15.689792 140257936017280 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0828 06:24:15.689963 140257936017280 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0828 06:24:16.679807 140257936017280 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0828 06:24:16.679975 140257936017280 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I0828 06:24:16.988680 140257936017280 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I0828 06:24:17.017257 140257936017280 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 28.65s\n",
            "I0828 06:24:17.130004 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 28.65s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I0828 06:24:17.135767 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0828 06:24:17.137405 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0828 06:24:17.137874 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0828 06:24:17.139250 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0828 06:24:17.140553 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0828 06:24:17.141010 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0828 06:24:17.141968 140257936017280 test_util.py:2459] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 33.946s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "#run model builder test\n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VA7Zbo3RLt3W"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path.\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "def plot_detections(image_np,\n",
        "                    boxes,\n",
        "                    classes,\n",
        "                    scores,\n",
        "                    category_index,\n",
        "                    figsize=(12, 16),\n",
        "                    image_name=None):\n",
        "  \"\"\"Wrapper function to visualize detections.\n",
        "\n",
        "  Args:\n",
        "    image_np: uint8 numpy array with shape (img_height, img_width, 3)\n",
        "    boxes: a numpy array of shape [N, 4]\n",
        "    classes: a numpy array of shape [N]. Note that class indices are 1-based,\n",
        "      and match the keys in the label map.\n",
        "    scores: a numpy array of shape [N] or None.  If scores=None, then\n",
        "      this function assumes that the boxes to be plotted are groundtruth\n",
        "      boxes and plot all boxes as black with no classes or scores.\n",
        "    category_index: a dict containing category dictionaries (each holding\n",
        "      category index `id` and category name `name`) keyed by category indices.\n",
        "    figsize: size for the figure.\n",
        "    image_name: a name for the image file.\n",
        "  \"\"\"\n",
        "  image_np_with_annotations = image_np.copy()\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_annotations,\n",
        "      boxes,\n",
        "      classes,\n",
        "      scores,\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      min_score_thresh=0.8)\n",
        "  if image_name:\n",
        "    plt.imsave(image_name, image_np_with_annotations)\n",
        "  else:\n",
        "    plt.imshow(image_np_with_annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPbU4I7aL9Fl"
      },
      "source": [
        "# Prepare Tensorflow 2 Object Detection Training Data\n",
        "\n",
        "\n",
        "Roboflow automatically creates our TFRecord and label_map files that we need!\n",
        "\n",
        "**Generating your own TFRecords the only step you need to change for your own custom dataset.**\n",
        "\n",
        "Because we need one TFRecord file for our training data, and one TFRecord file for our test data, we'll create two separate datasets in Roboflow and generate one set of TFRecords for each.\n",
        "\n",
        "To create a dataset in Roboflow and generate TFRecords, follow [this step-by-step guide](https://blog.roboflow.ai/getting-started-with-roboflow/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIREg_YwDa7-"
      },
      "source": [
        "![](https://i.imgur.com/ZwMdcbY.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX5sS8w1QnoG",
        "outputId": "79f45806-8852-40b6-a2f8-a8f9180ef6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 145 kB 41.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 178 kB 64.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 69.3 MB/s \n",
            "\u001b[?25h  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=tfrecord&ref=roboflow-tf2-od\n"
          ]
        }
      ],
      "source": [
        "#follow the link below to get your download code from from Roboflow\n",
        "!pip install -q roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"tfrecord\", notebook=\"roboflow-tf2-od\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcHJuaurS_AO",
        "outputId": "aab1c1fc-25ea-4a90-f627-34be85f8a178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.2.14)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.3.1)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.9.1)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.20.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.0)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Project77777-2 to tfrecord: 100% [182595817 / 182595817] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Project77777-2 in tfrecord:: 100%|██████████| 11/11 [00:01<00:00,  9.41it/s]\n"
          ]
        }
      ],
      "source": [
        "#Downloading data from Roboflow\n",
        "# from roboflow import Roboflow\n",
        "# rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
        "# project = rf.workspace().project(\"YOUR_PROJECT\")\n",
        "# dataset = project.version(\"YOUR_VERSION\").download(\"tfrecord\")\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"yRJZfBASNfzreOq4NOSP\")\n",
        "project = rf.workspace(\"sjh-a3lsu\").project(\"project77777\")\n",
        "dataset = project.version(2).download(\"tfrecord\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YUd2wtfrqedy"
      },
      "outputs": [],
      "source": [
        "# NOTE: Update these TFRecord names from \"cells\" and \"cells_label_map\" to your files!\n",
        "test_record_fname = dataset.location + '/test/Car-Person-Pet-Bike-Bicycle.tfrecord'\n",
        "train_record_fname = dataset.location + '/train/Car-Person-Pet-Bike-Bicycle.tfrecord'\n",
        "label_map_pbtxt_fname = dataset.location + '/train/Car-Person-Pet-Bike-Bicycle_label_map.pbtxt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2MAcgJ53STW"
      },
      "source": [
        "# Configure Custom TensorFlow2 Object Detection Training Configuration\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> In this section you can specify any model in the [TF2 OD model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) and set up your training configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gN0EUEa3e5Un"
      },
      "outputs": [],
      "source": [
        "##change chosen model to deploy different models available in the TF2 object detection zoo\n",
        "MODELS_CONFIG = {\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "    'efficientdet-d1': {\n",
        "        'model_name': 'efficientdet_d1_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d1_640x640_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d1_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "    'efficientdet-d2': {\n",
        "        'model_name': 'efficientdet_d2_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d2_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    },\n",
        "        'efficientdet-d3': {\n",
        "        'model_name': 'efficientdet_d3_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d3_896x896_coco17_tpu-32.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d3_coco17_tpu-32.tar.gz',\n",
        "        'batch_size': 16\n",
        "    }\n",
        "}\n",
        "\n",
        "#in this tutorial we implement the lightweight, smallest state of the art efficientdet model\n",
        "#if you want to scale up tot larger efficientdet models you will likely need more compute!\n",
        "chosen_model = 'efficientdet-d0'\n",
        "\n",
        "num_steps = 40000 #The more steps, the longer the training. Increase if your loss function is still decreasing and validation metrics are increasing. \n",
        "num_eval_steps = 500 #Perform evaluation after so many steps\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n",
        "batch_size = MODELS_CONFIG[chosen_model]['batch_size'] #if you can fit a large batch in memory, it may speed up your training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG4TmJUVrYQ7",
        "outputId": "d7dc3db5-893d-45ea-90d1-03b493d98f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/deploy\n",
            "--2022-08-28 06:25:45--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 74.125.24.128, 2404:6800:4003:c04::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|74.125.24.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30736482 (29M) [application/x-tar]\n",
            "Saving to: ‘efficientdet_d0_coco17_tpu-32.tar.gz’\n",
            "\n",
            "efficientdet_d0_coc 100%[===================>]  29.31M  21.7MB/s    in 1.3s    \n",
            "\n",
            "2022-08-28 06:25:47 (21.7 MB/s) - ‘efficientdet_d0_coco17_tpu-32.tar.gz’ saved [30736482/30736482]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#download pretrained weights\n",
        "%mkdir /content/models/research/deploy/\n",
        "%cd /content/models/research/deploy/\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-nqYZtdtsgG",
        "outputId": "0bfa264e-d35f-4159-dc76-eabc432002d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/deploy\n",
            "--2022-08-28 06:25:48--  https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4630 (4.5K) [text/plain]\n",
            "Saving to: ‘ssd_efficientdet_d0_512x512_coco17_tpu-8.config’\n",
            "\n",
            "ssd_efficientdet_d0 100%[===================>]   4.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-08-28 06:25:48 (57.0 MB/s) - ‘ssd_efficientdet_d0_512x512_coco17_tpu-8.config’ saved [4630/4630]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#download base training configuration file\n",
        "%cd /content/models/research/deploy\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b_ki9jOqxn7V"
      },
      "outputs": [],
      "source": [
        "#prepare\n",
        "pipeline_fname = '/content/models/research/deploy/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/research/deploy/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eA5ht3_yukT",
        "outputId": "7e94e795-7db7-4478-89e8-e3e279cd67ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/deploy\n",
            "writing custom configuration file\n"
          ]
        }
      ],
      "source": [
        "#write custom configuration file by slotting our dataset, model checkpoint, and training parameters into the base pipeline file\n",
        "\n",
        "import re\n",
        "\n",
        "%cd /content/models/research/deploy\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    \n",
        "    #fine-tune checkpoint type\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "        \n",
        "    f.write(s)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEsOLOMHzBqF",
        "outputId": "af89f515-0e92-4a0b-d93c-35d16936f582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " # SSD with EfficientNet-b0 + BiFPN feature extractor,\n",
            "# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n",
            "# See EfficientDet, Tan et al, https://arxiv.org/abs/1911.09070\n",
            "# See Lin et al, https://arxiv.org/abs/1708.02002\n",
            "# Trained on COCO, initialized from an EfficientNet-b0 checkpoint.\n",
            "#\n",
            "# Train on TPU-8\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    inplace_batchnorm_update: true\n",
            "    freeze_batchnorm: false\n",
            "    num_classes: 8\n",
            "    add_background_class: false\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "        use_matmul_gather: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    encode_background_as_zeros: true\n",
            "    anchor_generator {\n",
            "      multiscale_anchor_generator {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        anchor_scale: 4.0\n",
            "        aspect_ratios: [1.0, 2.0, 0.5]\n",
            "        scales_per_octave: 3\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      keep_aspect_ratio_resizer {\n",
            "        min_dimension: 512\n",
            "        max_dimension: 512\n",
            "        pad_to_max_dimension: true\n",
            "        }\n",
            "    }\n",
            "    box_predictor {\n",
            "      weight_shared_convolutional_box_predictor {\n",
            "        depth: 64\n",
            "        class_prediction_bias_init: -4.6\n",
            "        conv_hyperparams {\n",
            "          force_use_bias: true\n",
            "          activation: SWISH\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            random_normal_initializer {\n",
            "              stddev: 0.01\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            scale: true\n",
            "            decay: 0.99\n",
            "            epsilon: 0.001\n",
            "          }\n",
            "        }\n",
            "        num_layers_before_predictor: 3\n",
            "        kernel_size: 3\n",
            "        use_depthwise: true\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_efficientnet-b0_bifpn_keras'\n",
            "      bifpn {\n",
            "        min_level: 3\n",
            "        max_level: 7\n",
            "        num_iterations: 3\n",
            "        num_filters: 64\n",
            "      }\n",
            "      conv_hyperparams {\n",
            "        force_use_bias: true\n",
            "        activation: SWISH\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          scale: true,\n",
            "          decay: 0.99,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid_focal {\n",
            "          alpha: 0.25\n",
            "          gamma: 1.5\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    normalize_loc_loss_by_codesize: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  fine_tune_checkpoint: \"/content/models/research/deploy/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n",
            "  fine_tune_checkpoint_version: V2\n",
            "  fine_tune_checkpoint_type: \"detection\"\n",
            "  batch_size: 16\n",
            "  sync_replicas: true\n",
            "  startup_delay_steps: 0\n",
            "  replicas_to_aggregate: 8\n",
            "  use_bfloat16: true\n",
            "  num_steps: 40000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    random_scale_crop_and_pad_to_square {\n",
            "      output_size: 512\n",
            "      scale_min: 0.1\n",
            "      scale_max: 2.0\n",
            "    }\n",
            "  }\n",
            "  optimizer {\n",
            "    momentum_optimizer: {\n",
            "      learning_rate: {\n",
            "        cosine_decay_learning_rate {\n",
            "          learning_rate_base: 8e-2\n",
            "          total_steps: 300000\n",
            "          warmup_learning_rate: .001\n",
            "          warmup_steps: 2500\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "    }\n",
            "    use_moving_average: false\n",
            "  }\n",
            "  max_number_of_boxes: 100\n",
            "  unpad_groundtruth_tensors: false\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  label_map_path: \"/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle_label_map.pbtxt\"\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle.tfrecord\"\n",
            "  }\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  metrics_set: \"coco_detection_metrics\"\n",
            "  use_moving_averages: false\n",
            "  batch_size: 16;\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  label_map_path: \"/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_epochs: 1\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/Project77777-2/test/Car-Person-Pet-Bike-Bicycle.tfrecord\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat /content/models/research/deploy/pipeline_file.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GMlaN3rs3zLe"
      },
      "outputs": [],
      "source": [
        "pipeline_file = '/content/models/research/deploy/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxPj_QV43qD5"
      },
      "source": [
        "# Train Custom TF2 Object Detector\n",
        "\n",
        "* pipeline_file: defined above in writing custom training configuration\n",
        "* model_dir: the location tensorboard logs and saved model checkpoints will save to\n",
        "* num_train_steps: how long to train for\n",
        "* num_eval_steps: perform eval on validation set after this many steps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTfZChVzzpZ",
        "outputId": "f22527a8-43ca-4b95-a38b-0c40735d13d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 06:25:53.909604: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I0828 06:25:53.915897 139732604204928 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 40000\n",
            "I0828 06:25:53.919958 139732604204928 config_util.py:552] Maybe overwriting train_steps: 40000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0828 06:25:53.920136 139732604204928 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "I0828 06:25:53.927188 139732604204928 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0828 06:25:53.927294 139732604204928 ssd_efficientnet_bifpn_feature_extractor.py:147] EfficientDet BiFPN num filters: 64\n",
            "I0828 06:25:53.927362 139732604204928 ssd_efficientnet_bifpn_feature_extractor.py:149] EfficientDet BiFPN num iterations: 3\n",
            "I0828 06:25:53.931706 139732604204928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.951130 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.952886 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.955514 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.956505 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.963192 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.966522 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.971986 139732604204928 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0828 06:25:53.972105 139732604204928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.985267 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.986122 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.987661 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:53.988512 139732604204928 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I0828 06:25:54.064988 139732604204928 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0828 06:25:54.065123 139732604204928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:25:54.348640 139732604204928 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0828 06:25:54.348834 139732604204928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:25:54.645208 139732604204928 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0828 06:25:54.645392 139732604204928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:25:55.010584 139732604204928 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0828 06:25:55.010750 139732604204928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:25:55.377601 139732604204928 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0828 06:25:55.377781 139732604204928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:25:55.860258 139732604204928 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0828 06:25:55.860447 139732604204928 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0828 06:25:55.983803 139732604204928 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0828 06:25:56.032549 139732604204928 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W0828 06:25:56.073441 139732604204928 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle.tfrecord']\n",
            "I0828 06:25:56.080991 139732604204928 dataset_builder.py:162] Reading unweighted datasets: ['/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle.tfrecord']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle.tfrecord']\n",
            "I0828 06:25:56.081221 139732604204928 dataset_builder.py:79] Reading record datasets for input file: ['/content/Project77777-2/train/Car-Person-Pet-Bike-Bicycle.tfrecord']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I0828 06:25:56.081326 139732604204928 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0828 06:25:56.081398 139732604204928 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W0828 06:25:56.083307 139732604204928 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0828 06:25:56.100813 139732604204928 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0828 06:26:08.183453 139732604204928 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0828 06:26:14.999649 139732604204928 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:450: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0828 06:26:55.354372 139727521138432 deprecation.py:560] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0828 06:27:05.411524 139727521138432 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0828 06:27:19.983964 139727521138432 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0828 06:27:32.654736 139727521138432 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "W0828 06:27:46.472207 139727521138432 utils.py:80] Gradients do not exist for variables ['top_bn/gamma:0', 'top_bn/beta:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "INFO:tensorflow:Step 100 per-step time 2.119s\n",
            "I0828 06:30:26.910015 139732604204928 model_lib_v2.py:707] Step 100 per-step time 2.119s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 1.1021776,\n",
            " 'Loss/localization_loss': 0.67461574,\n",
            " 'Loss/regularization_loss': 0.02834446,\n",
            " 'Loss/total_loss': 1.8051379,\n",
            " 'learning_rate': 0.00416}\n",
            "I0828 06:30:26.910416 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 1.1021776,\n",
            " 'Loss/localization_loss': 0.67461574,\n",
            " 'Loss/regularization_loss': 0.02834446,\n",
            " 'Loss/total_loss': 1.8051379,\n",
            " 'learning_rate': 0.00416}\n",
            "INFO:tensorflow:Step 200 per-step time 1.277s\n",
            "I0828 06:32:34.551693 139732604204928 model_lib_v2.py:707] Step 200 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.8471155,\n",
            " 'Loss/localization_loss': 0.41251093,\n",
            " 'Loss/regularization_loss': 0.028358392,\n",
            " 'Loss/total_loss': 1.2879848,\n",
            " 'learning_rate': 0.0073200003}\n",
            "I0828 06:32:34.552050 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.8471155,\n",
            " 'Loss/localization_loss': 0.41251093,\n",
            " 'Loss/regularization_loss': 0.028358392,\n",
            " 'Loss/total_loss': 1.2879848,\n",
            " 'learning_rate': 0.0073200003}\n",
            "INFO:tensorflow:Step 300 per-step time 1.280s\n",
            "I0828 06:34:42.533213 139732604204928 model_lib_v2.py:707] Step 300 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.6928989,\n",
            " 'Loss/localization_loss': 0.3057482,\n",
            " 'Loss/regularization_loss': 0.028387645,\n",
            " 'Loss/total_loss': 1.0270348,\n",
            " 'learning_rate': 0.010480001}\n",
            "I0828 06:34:42.533559 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.6928989,\n",
            " 'Loss/localization_loss': 0.3057482,\n",
            " 'Loss/regularization_loss': 0.028387645,\n",
            " 'Loss/total_loss': 1.0270348,\n",
            " 'learning_rate': 0.010480001}\n",
            "INFO:tensorflow:Step 400 per-step time 1.279s\n",
            "I0828 06:36:50.407188 139732604204928 model_lib_v2.py:707] Step 400 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.5684848,\n",
            " 'Loss/localization_loss': 0.22311307,\n",
            " 'Loss/regularization_loss': 0.02843,\n",
            " 'Loss/total_loss': 0.8200278,\n",
            " 'learning_rate': 0.0136400005}\n",
            "I0828 06:36:50.407530 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.5684848,\n",
            " 'Loss/localization_loss': 0.22311307,\n",
            " 'Loss/regularization_loss': 0.02843,\n",
            " 'Loss/total_loss': 0.8200278,\n",
            " 'learning_rate': 0.0136400005}\n",
            "INFO:tensorflow:Step 500 per-step time 1.273s\n",
            "I0828 06:38:57.725682 139732604204928 model_lib_v2.py:707] Step 500 per-step time 1.273s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.53778684,\n",
            " 'Loss/localization_loss': 0.25682274,\n",
            " 'Loss/regularization_loss': 0.0284889,\n",
            " 'Loss/total_loss': 0.8230984,\n",
            " 'learning_rate': 0.016800001}\n",
            "I0828 06:38:57.726012 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.53778684,\n",
            " 'Loss/localization_loss': 0.25682274,\n",
            " 'Loss/regularization_loss': 0.0284889,\n",
            " 'Loss/total_loss': 0.8230984,\n",
            " 'learning_rate': 0.016800001}\n",
            "INFO:tensorflow:Step 600 per-step time 1.274s\n",
            "I0828 06:41:05.097212 139732604204928 model_lib_v2.py:707] Step 600 per-step time 1.274s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.50121826,\n",
            " 'Loss/localization_loss': 0.14388467,\n",
            " 'Loss/regularization_loss': 0.028573636,\n",
            " 'Loss/total_loss': 0.67367655,\n",
            " 'learning_rate': 0.019960001}\n",
            "I0828 06:41:05.097576 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.50121826,\n",
            " 'Loss/localization_loss': 0.14388467,\n",
            " 'Loss/regularization_loss': 0.028573636,\n",
            " 'Loss/total_loss': 0.67367655,\n",
            " 'learning_rate': 0.019960001}\n",
            "INFO:tensorflow:Step 700 per-step time 1.277s\n",
            "I0828 06:43:12.820084 139732604204928 model_lib_v2.py:707] Step 700 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.41580644,\n",
            " 'Loss/localization_loss': 0.21825425,\n",
            " 'Loss/regularization_loss': 0.028691022,\n",
            " 'Loss/total_loss': 0.66275173,\n",
            " 'learning_rate': 0.023120001}\n",
            "I0828 06:43:12.820496 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.41580644,\n",
            " 'Loss/localization_loss': 0.21825425,\n",
            " 'Loss/regularization_loss': 0.028691022,\n",
            " 'Loss/total_loss': 0.66275173,\n",
            " 'learning_rate': 0.023120001}\n",
            "INFO:tensorflow:Step 800 per-step time 1.275s\n",
            "I0828 06:45:20.354113 139732604204928 model_lib_v2.py:707] Step 800 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.4531273,\n",
            " 'Loss/localization_loss': 0.31810883,\n",
            " 'Loss/regularization_loss': 0.028821254,\n",
            " 'Loss/total_loss': 0.8000574,\n",
            " 'learning_rate': 0.02628}\n",
            "I0828 06:45:20.354447 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.4531273,\n",
            " 'Loss/localization_loss': 0.31810883,\n",
            " 'Loss/regularization_loss': 0.028821254,\n",
            " 'Loss/total_loss': 0.8000574,\n",
            " 'learning_rate': 0.02628}\n",
            "INFO:tensorflow:Step 900 per-step time 1.276s\n",
            "I0828 06:47:28.002975 139732604204928 model_lib_v2.py:707] Step 900 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.41254243,\n",
            " 'Loss/localization_loss': 0.1932442,\n",
            " 'Loss/regularization_loss': 0.02898606,\n",
            " 'Loss/total_loss': 0.6347727,\n",
            " 'learning_rate': 0.02944}\n",
            "I0828 06:47:28.003309 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.41254243,\n",
            " 'Loss/localization_loss': 0.1932442,\n",
            " 'Loss/regularization_loss': 0.02898606,\n",
            " 'Loss/total_loss': 0.6347727,\n",
            " 'learning_rate': 0.02944}\n",
            "INFO:tensorflow:Step 1000 per-step time 1.275s\n",
            "I0828 06:49:35.540440 139732604204928 model_lib_v2.py:707] Step 1000 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.43046486,\n",
            " 'Loss/localization_loss': 0.19335699,\n",
            " 'Loss/regularization_loss': 0.029172549,\n",
            " 'Loss/total_loss': 0.6529944,\n",
            " 'learning_rate': 0.0326}\n",
            "I0828 06:49:35.540762 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.43046486,\n",
            " 'Loss/localization_loss': 0.19335699,\n",
            " 'Loss/regularization_loss': 0.029172549,\n",
            " 'Loss/total_loss': 0.6529944,\n",
            " 'learning_rate': 0.0326}\n",
            "INFO:tensorflow:Step 1100 per-step time 1.280s\n",
            "I0828 06:51:43.491865 139732604204928 model_lib_v2.py:707] Step 1100 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.34820125,\n",
            " 'Loss/localization_loss': 0.16275229,\n",
            " 'Loss/regularization_loss': 0.029368091,\n",
            " 'Loss/total_loss': 0.54032165,\n",
            " 'learning_rate': 0.03576}\n",
            "I0828 06:51:43.492189 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.34820125,\n",
            " 'Loss/localization_loss': 0.16275229,\n",
            " 'Loss/regularization_loss': 0.029368091,\n",
            " 'Loss/total_loss': 0.54032165,\n",
            " 'learning_rate': 0.03576}\n",
            "INFO:tensorflow:Step 1200 per-step time 1.275s\n",
            "I0828 06:53:50.994475 139732604204928 model_lib_v2.py:707] Step 1200 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.36985213,\n",
            " 'Loss/localization_loss': 0.22905512,\n",
            " 'Loss/regularization_loss': 0.02958394,\n",
            " 'Loss/total_loss': 0.62849116,\n",
            " 'learning_rate': 0.03892}\n",
            "I0828 06:53:50.994798 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.36985213,\n",
            " 'Loss/localization_loss': 0.22905512,\n",
            " 'Loss/regularization_loss': 0.02958394,\n",
            " 'Loss/total_loss': 0.62849116,\n",
            " 'learning_rate': 0.03892}\n",
            "INFO:tensorflow:Step 1300 per-step time 1.271s\n",
            "I0828 06:55:58.053551 139732604204928 model_lib_v2.py:707] Step 1300 per-step time 1.271s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.37756398,\n",
            " 'Loss/localization_loss': 0.21704544,\n",
            " 'Loss/regularization_loss': 0.029841553,\n",
            " 'Loss/total_loss': 0.624451,\n",
            " 'learning_rate': 0.04208}\n",
            "I0828 06:55:58.053865 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.37756398,\n",
            " 'Loss/localization_loss': 0.21704544,\n",
            " 'Loss/regularization_loss': 0.029841553,\n",
            " 'Loss/total_loss': 0.624451,\n",
            " 'learning_rate': 0.04208}\n",
            "INFO:tensorflow:Step 1400 per-step time 1.281s\n",
            "I0828 06:58:06.176567 139732604204928 model_lib_v2.py:707] Step 1400 per-step time 1.281s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.43508196,\n",
            " 'Loss/localization_loss': 0.1903863,\n",
            " 'Loss/regularization_loss': 0.030123778,\n",
            " 'Loss/total_loss': 0.655592,\n",
            " 'learning_rate': 0.04524}\n",
            "I0828 06:58:06.176880 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.43508196,\n",
            " 'Loss/localization_loss': 0.1903863,\n",
            " 'Loss/regularization_loss': 0.030123778,\n",
            " 'Loss/total_loss': 0.655592,\n",
            " 'learning_rate': 0.04524}\n",
            "INFO:tensorflow:Step 1500 per-step time 1.279s\n",
            "I0828 07:00:14.101584 139732604204928 model_lib_v2.py:707] Step 1500 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.38629767,\n",
            " 'Loss/localization_loss': 0.19488248,\n",
            " 'Loss/regularization_loss': 0.030442242,\n",
            " 'Loss/total_loss': 0.6116224,\n",
            " 'learning_rate': 0.0484}\n",
            "I0828 07:00:14.101949 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.38629767,\n",
            " 'Loss/localization_loss': 0.19488248,\n",
            " 'Loss/regularization_loss': 0.030442242,\n",
            " 'Loss/total_loss': 0.6116224,\n",
            " 'learning_rate': 0.0484}\n",
            "INFO:tensorflow:Step 1600 per-step time 1.272s\n",
            "I0828 07:02:21.262214 139732604204928 model_lib_v2.py:707] Step 1600 per-step time 1.272s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.36543888,\n",
            " 'Loss/localization_loss': 0.15943575,\n",
            " 'Loss/regularization_loss': 0.030775776,\n",
            " 'Loss/total_loss': 0.5556504,\n",
            " 'learning_rate': 0.05156}\n",
            "I0828 07:02:21.262525 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.36543888,\n",
            " 'Loss/localization_loss': 0.15943575,\n",
            " 'Loss/regularization_loss': 0.030775776,\n",
            " 'Loss/total_loss': 0.5556504,\n",
            " 'learning_rate': 0.05156}\n",
            "INFO:tensorflow:Step 1700 per-step time 1.278s\n",
            "I0828 07:04:29.038151 139732604204928 model_lib_v2.py:707] Step 1700 per-step time 1.278s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.39051113,\n",
            " 'Loss/localization_loss': 0.19892903,\n",
            " 'Loss/regularization_loss': 0.031115212,\n",
            " 'Loss/total_loss': 0.62055534,\n",
            " 'learning_rate': 0.05472}\n",
            "I0828 07:04:29.038472 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.39051113,\n",
            " 'Loss/localization_loss': 0.19892903,\n",
            " 'Loss/regularization_loss': 0.031115212,\n",
            " 'Loss/total_loss': 0.62055534,\n",
            " 'learning_rate': 0.05472}\n",
            "INFO:tensorflow:Step 1800 per-step time 1.277s\n",
            "I0828 07:06:36.753033 139732604204928 model_lib_v2.py:707] Step 1800 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.41834635,\n",
            " 'Loss/localization_loss': 0.24814929,\n",
            " 'Loss/regularization_loss': 0.031525537,\n",
            " 'Loss/total_loss': 0.6980212,\n",
            " 'learning_rate': 0.05788}\n",
            "I0828 07:06:36.753362 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.41834635,\n",
            " 'Loss/localization_loss': 0.24814929,\n",
            " 'Loss/regularization_loss': 0.031525537,\n",
            " 'Loss/total_loss': 0.6980212,\n",
            " 'learning_rate': 0.05788}\n",
            "INFO:tensorflow:Step 1900 per-step time 1.273s\n",
            "I0828 07:08:44.020410 139732604204928 model_lib_v2.py:707] Step 1900 per-step time 1.273s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3020858,\n",
            " 'Loss/localization_loss': 0.16183314,\n",
            " 'Loss/regularization_loss': 0.031917803,\n",
            " 'Loss/total_loss': 0.49583673,\n",
            " 'learning_rate': 0.06104}\n",
            "I0828 07:08:44.020699 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.3020858,\n",
            " 'Loss/localization_loss': 0.16183314,\n",
            " 'Loss/regularization_loss': 0.031917803,\n",
            " 'Loss/total_loss': 0.49583673,\n",
            " 'learning_rate': 0.06104}\n",
            "INFO:tensorflow:Step 2000 per-step time 1.273s\n",
            "I0828 07:10:51.320247 139732604204928 model_lib_v2.py:707] Step 2000 per-step time 1.273s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.31685665,\n",
            " 'Loss/localization_loss': 0.13690752,\n",
            " 'Loss/regularization_loss': 0.032284603,\n",
            " 'Loss/total_loss': 0.48604876,\n",
            " 'learning_rate': 0.06420001}\n",
            "I0828 07:10:51.320540 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.31685665,\n",
            " 'Loss/localization_loss': 0.13690752,\n",
            " 'Loss/regularization_loss': 0.032284603,\n",
            " 'Loss/total_loss': 0.48604876,\n",
            " 'learning_rate': 0.06420001}\n",
            "INFO:tensorflow:Step 2100 per-step time 1.283s\n",
            "I0828 07:12:59.571124 139732604204928 model_lib_v2.py:707] Step 2100 per-step time 1.283s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33891588,\n",
            " 'Loss/localization_loss': 0.20103426,\n",
            " 'Loss/regularization_loss': 0.032703187,\n",
            " 'Loss/total_loss': 0.57265335,\n",
            " 'learning_rate': 0.067360006}\n",
            "I0828 07:12:59.571423 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.33891588,\n",
            " 'Loss/localization_loss': 0.20103426,\n",
            " 'Loss/regularization_loss': 0.032703187,\n",
            " 'Loss/total_loss': 0.57265335,\n",
            " 'learning_rate': 0.067360006}\n",
            "INFO:tensorflow:Step 2200 per-step time 1.280s\n",
            "I0828 07:15:07.595623 139732604204928 model_lib_v2.py:707] Step 2200 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.40419275,\n",
            " 'Loss/localization_loss': 0.19621931,\n",
            " 'Loss/regularization_loss': 0.033115994,\n",
            " 'Loss/total_loss': 0.63352805,\n",
            " 'learning_rate': 0.070520006}\n",
            "I0828 07:15:07.595912 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.40419275,\n",
            " 'Loss/localization_loss': 0.19621931,\n",
            " 'Loss/regularization_loss': 0.033115994,\n",
            " 'Loss/total_loss': 0.63352805,\n",
            " 'learning_rate': 0.070520006}\n",
            "INFO:tensorflow:Step 2300 per-step time 1.270s\n",
            "I0828 07:17:14.621099 139732604204928 model_lib_v2.py:707] Step 2300 per-step time 1.270s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3701111,\n",
            " 'Loss/localization_loss': 0.235789,\n",
            " 'Loss/regularization_loss': 0.033518005,\n",
            " 'Loss/total_loss': 0.6394181,\n",
            " 'learning_rate': 0.073680006}\n",
            "I0828 07:17:14.621401 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.3701111,\n",
            " 'Loss/localization_loss': 0.235789,\n",
            " 'Loss/regularization_loss': 0.033518005,\n",
            " 'Loss/total_loss': 0.6394181,\n",
            " 'learning_rate': 0.073680006}\n",
            "INFO:tensorflow:Step 2400 per-step time 1.277s\n",
            "I0828 07:19:22.316445 139732604204928 model_lib_v2.py:707] Step 2400 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.41089764,\n",
            " 'Loss/localization_loss': 0.16293733,\n",
            " 'Loss/regularization_loss': 0.03401143,\n",
            " 'Loss/total_loss': 0.6078464,\n",
            " 'learning_rate': 0.076840006}\n",
            "I0828 07:19:22.316741 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.41089764,\n",
            " 'Loss/localization_loss': 0.16293733,\n",
            " 'Loss/regularization_loss': 0.03401143,\n",
            " 'Loss/total_loss': 0.6078464,\n",
            " 'learning_rate': 0.076840006}\n",
            "INFO:tensorflow:Step 2500 per-step time 1.276s\n",
            "I0828 07:21:29.948665 139732604204928 model_lib_v2.py:707] Step 2500 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3380211,\n",
            " 'Loss/localization_loss': 0.17146401,\n",
            " 'Loss/regularization_loss': 0.03451328,\n",
            " 'Loss/total_loss': 0.5439984,\n",
            " 'learning_rate': 0.08}\n",
            "I0828 07:21:29.948951 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.3380211,\n",
            " 'Loss/localization_loss': 0.17146401,\n",
            " 'Loss/regularization_loss': 0.03451328,\n",
            " 'Loss/total_loss': 0.5439984,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 2600 per-step time 1.275s\n",
            "I0828 07:23:37.486803 139732604204928 model_lib_v2.py:707] Step 2600 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26231524,\n",
            " 'Loss/localization_loss': 0.18781932,\n",
            " 'Loss/regularization_loss': 0.03500951,\n",
            " 'Loss/total_loss': 0.48514408,\n",
            " 'learning_rate': 0.079999976}\n",
            "I0828 07:23:37.487125 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.26231524,\n",
            " 'Loss/localization_loss': 0.18781932,\n",
            " 'Loss/regularization_loss': 0.03500951,\n",
            " 'Loss/total_loss': 0.48514408,\n",
            " 'learning_rate': 0.079999976}\n",
            "INFO:tensorflow:Step 2700 per-step time 1.276s\n",
            "I0828 07:25:45.082996 139732604204928 model_lib_v2.py:707] Step 2700 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.36398107,\n",
            " 'Loss/localization_loss': 0.17128903,\n",
            " 'Loss/regularization_loss': 0.03552137,\n",
            " 'Loss/total_loss': 0.5707915,\n",
            " 'learning_rate': 0.07999991}\n",
            "I0828 07:25:45.083307 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.36398107,\n",
            " 'Loss/localization_loss': 0.17128903,\n",
            " 'Loss/regularization_loss': 0.03552137,\n",
            " 'Loss/total_loss': 0.5707915,\n",
            " 'learning_rate': 0.07999991}\n",
            "INFO:tensorflow:Step 2800 per-step time 1.280s\n",
            "I0828 07:27:53.101701 139732604204928 model_lib_v2.py:707] Step 2800 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.39797267,\n",
            " 'Loss/localization_loss': 0.15028669,\n",
            " 'Loss/regularization_loss': 0.035936385,\n",
            " 'Loss/total_loss': 0.58419573,\n",
            " 'learning_rate': 0.0799998}\n",
            "I0828 07:27:53.102002 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.39797267,\n",
            " 'Loss/localization_loss': 0.15028669,\n",
            " 'Loss/regularization_loss': 0.035936385,\n",
            " 'Loss/total_loss': 0.58419573,\n",
            " 'learning_rate': 0.0799998}\n",
            "INFO:tensorflow:Step 2900 per-step time 1.268s\n",
            "I0828 07:29:59.872843 139732604204928 model_lib_v2.py:707] Step 2900 per-step time 1.268s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33255526,\n",
            " 'Loss/localization_loss': 0.13162257,\n",
            " 'Loss/regularization_loss': 0.03632109,\n",
            " 'Loss/total_loss': 0.5004989,\n",
            " 'learning_rate': 0.07999964}\n",
            "I0828 07:29:59.873152 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.33255526,\n",
            " 'Loss/localization_loss': 0.13162257,\n",
            " 'Loss/regularization_loss': 0.03632109,\n",
            " 'Loss/total_loss': 0.5004989,\n",
            " 'learning_rate': 0.07999964}\n",
            "INFO:tensorflow:Step 3000 per-step time 1.279s\n",
            "I0828 07:32:07.753043 139732604204928 model_lib_v2.py:707] Step 3000 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.30414665,\n",
            " 'Loss/localization_loss': 0.16353118,\n",
            " 'Loss/regularization_loss': 0.036706477,\n",
            " 'Loss/total_loss': 0.5043843,\n",
            " 'learning_rate': 0.07999944}\n",
            "I0828 07:32:07.753348 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.30414665,\n",
            " 'Loss/localization_loss': 0.16353118,\n",
            " 'Loss/regularization_loss': 0.036706477,\n",
            " 'Loss/total_loss': 0.5043843,\n",
            " 'learning_rate': 0.07999944}\n",
            "INFO:tensorflow:Step 3100 per-step time 1.286s\n",
            "I0828 07:34:16.366811 139732604204928 model_lib_v2.py:707] Step 3100 per-step time 1.286s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33545408,\n",
            " 'Loss/localization_loss': 0.21457121,\n",
            " 'Loss/regularization_loss': 0.03707797,\n",
            " 'Loss/total_loss': 0.58710325,\n",
            " 'learning_rate': 0.07999919}\n",
            "I0828 07:34:16.367134 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.33545408,\n",
            " 'Loss/localization_loss': 0.21457121,\n",
            " 'Loss/regularization_loss': 0.03707797,\n",
            " 'Loss/total_loss': 0.58710325,\n",
            " 'learning_rate': 0.07999919}\n",
            "INFO:tensorflow:Step 3200 per-step time 1.276s\n",
            "I0828 07:36:23.979297 139732604204928 model_lib_v2.py:707] Step 3200 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.73853236,\n",
            " 'Loss/localization_loss': 0.33208483,\n",
            " 'Loss/regularization_loss': 0.03760625,\n",
            " 'Loss/total_loss': 1.1082234,\n",
            " 'learning_rate': 0.0799989}\n",
            "I0828 07:36:23.979593 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.73853236,\n",
            " 'Loss/localization_loss': 0.33208483,\n",
            " 'Loss/regularization_loss': 0.03760625,\n",
            " 'Loss/total_loss': 1.1082234,\n",
            " 'learning_rate': 0.0799989}\n",
            "INFO:tensorflow:Step 3300 per-step time 1.275s\n",
            "I0828 07:38:31.436198 139732604204928 model_lib_v2.py:707] Step 3300 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.35786298,\n",
            " 'Loss/localization_loss': 0.18584432,\n",
            " 'Loss/regularization_loss': 0.037975136,\n",
            " 'Loss/total_loss': 0.58168244,\n",
            " 'learning_rate': 0.07999857}\n",
            "I0828 07:38:31.436511 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.35786298,\n",
            " 'Loss/localization_loss': 0.18584432,\n",
            " 'Loss/regularization_loss': 0.037975136,\n",
            " 'Loss/total_loss': 0.58168244,\n",
            " 'learning_rate': 0.07999857}\n",
            "INFO:tensorflow:Step 3400 per-step time 1.274s\n",
            "I0828 07:40:38.884456 139732604204928 model_lib_v2.py:707] Step 3400 per-step time 1.274s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.35245442,\n",
            " 'Loss/localization_loss': 0.1879406,\n",
            " 'Loss/regularization_loss': 0.038267054,\n",
            " 'Loss/total_loss': 0.57866204,\n",
            " 'learning_rate': 0.07999819}\n",
            "I0828 07:40:38.884763 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.35245442,\n",
            " 'Loss/localization_loss': 0.1879406,\n",
            " 'Loss/regularization_loss': 0.038267054,\n",
            " 'Loss/total_loss': 0.57866204,\n",
            " 'learning_rate': 0.07999819}\n",
            "INFO:tensorflow:Step 3500 per-step time 1.273s\n",
            "I0828 07:42:46.225961 139732604204928 model_lib_v2.py:707] Step 3500 per-step time 1.273s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.31474173,\n",
            " 'Loss/localization_loss': 0.13849874,\n",
            " 'Loss/regularization_loss': 0.038566314,\n",
            " 'Loss/total_loss': 0.4918068,\n",
            " 'learning_rate': 0.07999776}\n",
            "I0828 07:42:46.227480 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.31474173,\n",
            " 'Loss/localization_loss': 0.13849874,\n",
            " 'Loss/regularization_loss': 0.038566314,\n",
            " 'Loss/total_loss': 0.4918068,\n",
            " 'learning_rate': 0.07999776}\n",
            "INFO:tensorflow:Step 3600 per-step time 1.277s\n",
            "I0828 07:44:53.884362 139732604204928 model_lib_v2.py:707] Step 3600 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.4067243,\n",
            " 'Loss/localization_loss': 0.25791693,\n",
            " 'Loss/regularization_loss': 0.03886969,\n",
            " 'Loss/total_loss': 0.7035109,\n",
            " 'learning_rate': 0.0799973}\n",
            "I0828 07:44:53.884649 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.4067243,\n",
            " 'Loss/localization_loss': 0.25791693,\n",
            " 'Loss/regularization_loss': 0.03886969,\n",
            " 'Loss/total_loss': 0.7035109,\n",
            " 'learning_rate': 0.0799973}\n",
            "INFO:tensorflow:Step 3700 per-step time 1.281s\n",
            "I0828 07:47:01.998304 139732604204928 model_lib_v2.py:707] Step 3700 per-step time 1.281s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23175628,\n",
            " 'Loss/localization_loss': 0.080559365,\n",
            " 'Loss/regularization_loss': 0.039199322,\n",
            " 'Loss/total_loss': 0.351515,\n",
            " 'learning_rate': 0.07999679}\n",
            "I0828 07:47:01.998592 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.23175628,\n",
            " 'Loss/localization_loss': 0.080559365,\n",
            " 'Loss/regularization_loss': 0.039199322,\n",
            " 'Loss/total_loss': 0.351515,\n",
            " 'learning_rate': 0.07999679}\n",
            "INFO:tensorflow:Step 3800 per-step time 1.279s\n",
            "I0828 07:49:09.866423 139732604204928 model_lib_v2.py:707] Step 3800 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33946726,\n",
            " 'Loss/localization_loss': 0.16929612,\n",
            " 'Loss/regularization_loss': 0.03947509,\n",
            " 'Loss/total_loss': 0.54823846,\n",
            " 'learning_rate': 0.07999623}\n",
            "I0828 07:49:09.866719 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.33946726,\n",
            " 'Loss/localization_loss': 0.16929612,\n",
            " 'Loss/regularization_loss': 0.03947509,\n",
            " 'Loss/total_loss': 0.54823846,\n",
            " 'learning_rate': 0.07999623}\n",
            "INFO:tensorflow:Step 3900 per-step time 1.274s\n",
            "I0828 07:51:17.239903 139732604204928 model_lib_v2.py:707] Step 3900 per-step time 1.274s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.34780705,\n",
            " 'Loss/localization_loss': 0.16061145,\n",
            " 'Loss/regularization_loss': 0.03977138,\n",
            " 'Loss/total_loss': 0.5481899,\n",
            " 'learning_rate': 0.07999563}\n",
            "I0828 07:51:17.240221 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.34780705,\n",
            " 'Loss/localization_loss': 0.16061145,\n",
            " 'Loss/regularization_loss': 0.03977138,\n",
            " 'Loss/total_loss': 0.5481899,\n",
            " 'learning_rate': 0.07999563}\n",
            "INFO:tensorflow:Step 4000 per-step time 1.276s\n",
            "I0828 07:53:24.793584 139732604204928 model_lib_v2.py:707] Step 4000 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.29799256,\n",
            " 'Loss/localization_loss': 0.16925219,\n",
            " 'Loss/regularization_loss': 0.040022627,\n",
            " 'Loss/total_loss': 0.50726736,\n",
            " 'learning_rate': 0.079994984}\n",
            "I0828 07:53:24.793892 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.29799256,\n",
            " 'Loss/localization_loss': 0.16925219,\n",
            " 'Loss/regularization_loss': 0.040022627,\n",
            " 'Loss/total_loss': 0.50726736,\n",
            " 'learning_rate': 0.079994984}\n",
            "INFO:tensorflow:Step 4100 per-step time 1.292s\n",
            "I0828 07:55:33.972146 139732604204928 model_lib_v2.py:707] Step 4100 per-step time 1.292s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.34218597,\n",
            " 'Loss/localization_loss': 0.22376947,\n",
            " 'Loss/regularization_loss': 0.0403088,\n",
            " 'Loss/total_loss': 0.60626423,\n",
            " 'learning_rate': 0.07999428}\n",
            "I0828 07:55:33.972461 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.34218597,\n",
            " 'Loss/localization_loss': 0.22376947,\n",
            " 'Loss/regularization_loss': 0.0403088,\n",
            " 'Loss/total_loss': 0.60626423,\n",
            " 'learning_rate': 0.07999428}\n",
            "INFO:tensorflow:Step 4200 per-step time 1.270s\n",
            "I0828 07:57:40.980115 139732604204928 model_lib_v2.py:707] Step 4200 per-step time 1.270s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.26714158,\n",
            " 'Loss/localization_loss': 0.087308414,\n",
            " 'Loss/regularization_loss': 0.040550068,\n",
            " 'Loss/total_loss': 0.39500004,\n",
            " 'learning_rate': 0.07999355}\n",
            "I0828 07:57:40.980417 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.26714158,\n",
            " 'Loss/localization_loss': 0.087308414,\n",
            " 'Loss/regularization_loss': 0.040550068,\n",
            " 'Loss/total_loss': 0.39500004,\n",
            " 'learning_rate': 0.07999355}\n",
            "INFO:tensorflow:Step 4300 per-step time 1.279s\n",
            "I0828 07:59:48.895382 139732604204928 model_lib_v2.py:707] Step 4300 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.23539025,\n",
            " 'Loss/localization_loss': 0.14568242,\n",
            " 'Loss/regularization_loss': 0.04082352,\n",
            " 'Loss/total_loss': 0.4218962,\n",
            " 'learning_rate': 0.07999277}\n",
            "I0828 07:59:48.895690 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.23539025,\n",
            " 'Loss/localization_loss': 0.14568242,\n",
            " 'Loss/regularization_loss': 0.04082352,\n",
            " 'Loss/total_loss': 0.4218962,\n",
            " 'learning_rate': 0.07999277}\n",
            "INFO:tensorflow:Step 4400 per-step time 1.279s\n",
            "I0828 08:01:56.770883 139732604204928 model_lib_v2.py:707] Step 4400 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2894187,\n",
            " 'Loss/localization_loss': 0.13123897,\n",
            " 'Loss/regularization_loss': 0.041024033,\n",
            " 'Loss/total_loss': 0.4616817,\n",
            " 'learning_rate': 0.07999195}\n",
            "I0828 08:01:56.771189 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.2894187,\n",
            " 'Loss/localization_loss': 0.13123897,\n",
            " 'Loss/regularization_loss': 0.041024033,\n",
            " 'Loss/total_loss': 0.4616817,\n",
            " 'learning_rate': 0.07999195}\n",
            "INFO:tensorflow:Step 4500 per-step time 1.279s\n",
            "I0828 08:04:04.676149 139732604204928 model_lib_v2.py:707] Step 4500 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.35407543,\n",
            " 'Loss/localization_loss': 0.14094205,\n",
            " 'Loss/regularization_loss': 0.04120888,\n",
            " 'Loss/total_loss': 0.53622633,\n",
            " 'learning_rate': 0.07999108}\n",
            "I0828 08:04:04.676456 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.35407543,\n",
            " 'Loss/localization_loss': 0.14094205,\n",
            " 'Loss/regularization_loss': 0.04120888,\n",
            " 'Loss/total_loss': 0.53622633,\n",
            " 'learning_rate': 0.07999108}\n",
            "INFO:tensorflow:Step 4600 per-step time 1.273s\n",
            "I0828 08:06:11.947215 139732604204928 model_lib_v2.py:707] Step 4600 per-step time 1.273s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2966729,\n",
            " 'Loss/localization_loss': 0.14957029,\n",
            " 'Loss/regularization_loss': 0.041463483,\n",
            " 'Loss/total_loss': 0.4877067,\n",
            " 'learning_rate': 0.07999016}\n",
            "I0828 08:06:11.947520 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.2966729,\n",
            " 'Loss/localization_loss': 0.14957029,\n",
            " 'Loss/regularization_loss': 0.041463483,\n",
            " 'Loss/total_loss': 0.4877067,\n",
            " 'learning_rate': 0.07999016}\n",
            "INFO:tensorflow:Step 4700 per-step time 1.280s\n",
            "I0828 08:08:19.936185 139732604204928 model_lib_v2.py:707] Step 4700 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.28972647,\n",
            " 'Loss/localization_loss': 0.15586613,\n",
            " 'Loss/regularization_loss': 0.041687287,\n",
            " 'Loss/total_loss': 0.4872799,\n",
            " 'learning_rate': 0.0799892}\n",
            "I0828 08:08:19.936491 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.28972647,\n",
            " 'Loss/localization_loss': 0.15586613,\n",
            " 'Loss/regularization_loss': 0.041687287,\n",
            " 'Loss/total_loss': 0.4872799,\n",
            " 'learning_rate': 0.0799892}\n",
            "INFO:tensorflow:Step 4800 per-step time 1.278s\n",
            "I0828 08:10:27.747029 139732604204928 model_lib_v2.py:707] Step 4800 per-step time 1.278s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.25551745,\n",
            " 'Loss/localization_loss': 0.061004974,\n",
            " 'Loss/regularization_loss': 0.041916545,\n",
            " 'Loss/total_loss': 0.35843897,\n",
            " 'learning_rate': 0.079988204}\n",
            "I0828 08:10:27.747344 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.25551745,\n",
            " 'Loss/localization_loss': 0.061004974,\n",
            " 'Loss/regularization_loss': 0.041916545,\n",
            " 'Loss/total_loss': 0.35843897,\n",
            " 'learning_rate': 0.079988204}\n",
            "INFO:tensorflow:Step 4900 per-step time 1.274s\n",
            "I0828 08:12:35.184499 139732604204928 model_lib_v2.py:707] Step 4900 per-step time 1.274s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.40807366,\n",
            " 'Loss/localization_loss': 0.2896207,\n",
            " 'Loss/regularization_loss': 0.042115554,\n",
            " 'Loss/total_loss': 0.73980993,\n",
            " 'learning_rate': 0.07998715}\n",
            "I0828 08:12:35.184791 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.40807366,\n",
            " 'Loss/localization_loss': 0.2896207,\n",
            " 'Loss/regularization_loss': 0.042115554,\n",
            " 'Loss/total_loss': 0.73980993,\n",
            " 'learning_rate': 0.07998715}\n",
            "INFO:tensorflow:Step 5000 per-step time 1.280s\n",
            "I0828 08:14:43.229999 139732604204928 model_lib_v2.py:707] Step 5000 per-step time 1.280s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.4109317,\n",
            " 'Loss/localization_loss': 0.28843647,\n",
            " 'Loss/regularization_loss': 0.04238985,\n",
            " 'Loss/total_loss': 0.741758,\n",
            " 'learning_rate': 0.07998606}\n",
            "I0828 08:14:43.230294 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.4109317,\n",
            " 'Loss/localization_loss': 0.28843647,\n",
            " 'Loss/regularization_loss': 0.04238985,\n",
            " 'Loss/total_loss': 0.741758,\n",
            " 'learning_rate': 0.07998606}\n",
            "INFO:tensorflow:Step 5100 per-step time 1.279s\n",
            "I0828 08:16:51.127178 139732604204928 model_lib_v2.py:707] Step 5100 per-step time 1.279s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.31031847,\n",
            " 'Loss/localization_loss': 0.1357773,\n",
            " 'Loss/regularization_loss': 0.042616244,\n",
            " 'Loss/total_loss': 0.488712,\n",
            " 'learning_rate': 0.07998492}\n",
            "I0828 08:16:51.127478 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.31031847,\n",
            " 'Loss/localization_loss': 0.1357773,\n",
            " 'Loss/regularization_loss': 0.042616244,\n",
            " 'Loss/total_loss': 0.488712,\n",
            " 'learning_rate': 0.07998492}\n",
            "INFO:tensorflow:Step 5200 per-step time 1.275s\n",
            "I0828 08:18:58.655891 139732604204928 model_lib_v2.py:707] Step 5200 per-step time 1.275s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.24724229,\n",
            " 'Loss/localization_loss': 0.119994745,\n",
            " 'Loss/regularization_loss': 0.042835068,\n",
            " 'Loss/total_loss': 0.4100721,\n",
            " 'learning_rate': 0.07998374}\n",
            "I0828 08:18:58.656266 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.24724229,\n",
            " 'Loss/localization_loss': 0.119994745,\n",
            " 'Loss/regularization_loss': 0.042835068,\n",
            " 'Loss/total_loss': 0.4100721,\n",
            " 'learning_rate': 0.07998374}\n",
            "INFO:tensorflow:Step 5300 per-step time 1.276s\n",
            "I0828 08:21:06.286378 139732604204928 model_lib_v2.py:707] Step 5300 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.21909085,\n",
            " 'Loss/localization_loss': 0.09885604,\n",
            " 'Loss/regularization_loss': 0.042996254,\n",
            " 'Loss/total_loss': 0.36094314,\n",
            " 'learning_rate': 0.07998252}\n",
            "I0828 08:21:06.286675 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.21909085,\n",
            " 'Loss/localization_loss': 0.09885604,\n",
            " 'Loss/regularization_loss': 0.042996254,\n",
            " 'Loss/total_loss': 0.36094314,\n",
            " 'learning_rate': 0.07998252}\n",
            "INFO:tensorflow:Step 5400 per-step time 1.276s\n",
            "I0828 08:23:13.914855 139732604204928 model_lib_v2.py:707] Step 5400 per-step time 1.276s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.33889222,\n",
            " 'Loss/localization_loss': 0.19390336,\n",
            " 'Loss/regularization_loss': 0.043187182,\n",
            " 'Loss/total_loss': 0.57598275,\n",
            " 'learning_rate': 0.079981245}\n",
            "I0828 08:23:13.915183 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.33889222,\n",
            " 'Loss/localization_loss': 0.19390336,\n",
            " 'Loss/regularization_loss': 0.043187182,\n",
            " 'Loss/total_loss': 0.57598275,\n",
            " 'learning_rate': 0.079981245}\n",
            "INFO:tensorflow:Step 5500 per-step time 1.277s\n",
            "I0828 08:25:21.660278 139732604204928 model_lib_v2.py:707] Step 5500 per-step time 1.277s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2677804,\n",
            " 'Loss/localization_loss': 0.120868385,\n",
            " 'Loss/regularization_loss': 0.04341412,\n",
            " 'Loss/total_loss': 0.4320629,\n",
            " 'learning_rate': 0.07997993}\n",
            "I0828 08:25:21.660614 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.2677804,\n",
            " 'Loss/localization_loss': 0.120868385,\n",
            " 'Loss/regularization_loss': 0.04341412,\n",
            " 'Loss/total_loss': 0.4320629,\n",
            " 'learning_rate': 0.07997993}\n",
            "INFO:tensorflow:Step 5600 per-step time 1.274s\n",
            "I0828 08:27:29.060552 139732604204928 model_lib_v2.py:707] Step 5600 per-step time 1.274s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.28470472,\n",
            " 'Loss/localization_loss': 0.15840226,\n",
            " 'Loss/regularization_loss': 0.043590233,\n",
            " 'Loss/total_loss': 0.4866972,\n",
            " 'learning_rate': 0.07997857}\n",
            "I0828 08:27:29.060920 139732604204928 model_lib_v2.py:708] {'Loss/classification_loss': 0.28470472,\n",
            " 'Loss/localization_loss': 0.15840226,\n",
            " 'Loss/regularization_loss': 0.043590233,\n",
            " 'Loss/total_loss': 0.4866972,\n",
            " 'learning_rate': 0.07997857}\n"
          ]
        }
      ],
      "source": [
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNv1N_hUibE"
      },
      "outputs": [],
      "source": [
        "#run model evaluation to obtain performance metrics\n",
        "#!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    #--pipeline_config_path={pipeline_file} \\\n",
        "    #--model_dir={model_dir} \\\n",
        "    #--checkpoint_dir={model_dir} \\\n",
        "#Not yet implemented for EfficientDet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI9iCCxoNlAL"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vk2146Ogil3"
      },
      "source": [
        "## Exporting a Trained Inference Graph\n",
        "Still to come for TF2 models, we will be updating this Colab notebook accordingly as the functionality is added. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqaZ4v-vIuDl"
      },
      "outputs": [],
      "source": [
        "#see where our model saved weights\n",
        "%ls '/content/training/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnSEZIzl4M10"
      },
      "outputs": [],
      "source": [
        "#run conversion script\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = '/content/fine_tuned_model'\n",
        "\n",
        "#place the model weights you would like to export here\n",
        "last_model_path = '/content/training/'\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsE_uVjlsz3u"
      },
      "outputs": [],
      "source": [
        "%ls '/content/fine_tuned_model/saved_model/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vz2vJeCCyZR"
      },
      "source": [
        "# Run Inference on Test Images with Custom TensorFlow2 Object Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcR4PWC3KBau"
      },
      "outputs": [],
      "source": [
        "#downloading test images from Roboflow\n",
        "#export dataset above with format COCO JSON\n",
        "#or import your test images via other means. \n",
        "%mkdir /content/test/\n",
        "%cd /content/test/\n",
        "!curl -L \"https://app.roboflow.com/ds/QD2sZRBF4i?key=4QwWn2TZEM\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxtm1NutE5vK"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs1HJnEhyevJ"
      },
      "outputs": [],
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f6DTolSDfXs"
      },
      "outputs": [],
      "source": [
        "%ls '/content/training/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFY75DfTDHaU"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "filenames = list(pathlib.Path('/content/training/').glob('*.index'))\n",
        "\n",
        "filenames.sort()\n",
        "print(filenames)\n",
        "\n",
        "#recover our saved model\n",
        "pipeline_config = pipeline_file\n",
        "#generally you want to put the last ckpt from training in here\n",
        "model_dir = str(filenames[-1]).replace('.index','')\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=False)\n",
        "\n",
        "# Restore checkpoint\n",
        "ckpt = tf.compat.v2.train.Checkpoint(\n",
        "      model=detection_model)\n",
        "ckpt.restore(os.path.join(str(filenames[-1]).replace('.index','')))\n",
        "\n",
        "\n",
        "def get_model_detection_function(model):\n",
        "  \"\"\"Get a tf.function for detection.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def detect_fn(image):\n",
        "    \"\"\"Detect objects in image.\"\"\"\n",
        "\n",
        "    image, shapes = model.preprocess(image)\n",
        "    prediction_dict = model.predict(image, shapes)\n",
        "    detections = model.postprocess(prediction_dict, shapes)\n",
        "\n",
        "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
        "\n",
        "  return detect_fn\n",
        "\n",
        "detect_fn = get_model_detection_function(detection_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ycfl7rnDT1D"
      },
      "outputs": [],
      "source": [
        "#map labels for inference decoding\n",
        "label_map_path = configs['eval_input_config'].label_map_path\n",
        "label_map = label_map_util.load_labelmap(label_map_path)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map,\n",
        "    max_num_classes=label_map_util.get_max_label_map_index(label_map),\n",
        "    use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "label_map_dict = label_map_util.get_label_map_dict(label_map, use_display_name=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN1BzORoIzV4"
      },
      "outputs": [],
      "source": [
        "#run detector on test image\n",
        "#it takes a little longer on the first run and then runs at normal speed. \n",
        "import random\n",
        "\n",
        "TEST_IMAGE_PATHS = glob.glob('/content/test/test/*.jpg')\n",
        "image_path = random.choice(TEST_IMAGE_PATHS)\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "# Things to try:\n",
        "# Flip horizontally\n",
        "# image_np = np.fliplr(image_np).copy()\n",
        "\n",
        "# Convert image to grayscale\n",
        "# image_np = np.tile(\n",
        "#     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\n",
        "\n",
        "input_tensor = tf.convert_to_tensor(\n",
        "    np.expand_dims(image_np, 0), dtype=tf.float32)\n",
        "detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
        "\n",
        "label_id_offset = 1\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections,\n",
        "      detections['detection_boxes'][0].numpy(),\n",
        "      (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
        "      detections['detection_scores'][0].numpy(),\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.5,\n",
        "      agnostic_mode=False,\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12,16))\n",
        "plt.imshow(image_np_with_detections)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-N94cKB82o"
      },
      "source": [
        "## Congrats!\n",
        "\n",
        "Hope you enjoyed this!\n",
        "\n",
        "--Team [Roboflow](https://roboflow.ai)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TF2 이거다!",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}